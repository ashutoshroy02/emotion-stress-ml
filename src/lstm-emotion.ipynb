{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":639622,"sourceType":"datasetVersion","datasetId":316368}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport librosa\nimport librosa.display\nfrom IPython.display import Audio\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"paths = []\nlabels = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        paths.append(os.path.join(dirname, filename))\n        label = filename.split('_')[-1]\n        label = label.split('.')[0]\n        labels.append(label.lower())\n    if len(paths) == 2800:\n        break\nprint('Dataset is Loaded')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(paths)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"paths[:5]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels[:5]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Create a dataframe\ndf = pd.DataFrame()\ndf['speech'] = paths\ndf['label'] = labels\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['label'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.countplot(data=df, x='label')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def waveplot(data, sr, emotion):\n    plt.figure(figsize=(10,4))\n    plt.title(emotion, size=20)\n    librosa.display.waveshow(data, sr=sr)\n    plt.show()\n    \ndef spectogram(data, sr, emotion):\n    x = librosa.stft(data)\n    xdb = librosa.amplitude_to_db(abs(x))\n    plt.figure(figsize=(11,4))\n    plt.title(emotion, size=20)\n    librosa.display.specshow(xdb, sr=sr, x_axis='time', y_axis='hz')\n    plt.colorbar()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion = 'fear'\npath = np.array(df['speech'][df['label']==emotion])[0]\ndata, sampling_rate = librosa.load(path)\nwaveplot(data, sampling_rate, emotion)\nspectogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion = 'angry'\npath = np.array(df['speech'][df['label']==emotion])[1]\ndata, sampling_rate = librosa.load(path)\nwaveplot(data, sampling_rate, emotion)\nspectogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion = 'disgust'\npath = np.array(df['speech'][df['label']==emotion])[0]\ndata, sampling_rate = librosa.load(path)\nwaveplot(data, sampling_rate, emotion)\nspectogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion = 'neutral'\npath = np.array(df['speech'][df['label']==emotion])[0]\ndata, sampling_rate = librosa.load(path)\nwaveplot(data, sampling_rate, emotion)\nspectogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion = 'sad'\npath = np.array(df['speech'][df['label']==emotion])[0]\ndata, sampling_rate = librosa.load(path)\nwaveplot(data, sampling_rate, emotion)\nspectogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion = 'ps'\npath = np.array(df['speech'][df['label']==emotion])[0]\ndata, sampling_rate = librosa.load(path)\nwaveplot(data, sampling_rate, emotion)\nspectogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion = 'happy'\npath = np.array(df['speech'][df['label']==emotion])[0]\ndata, sampling_rate = librosa.load(path)\nwaveplot(data, sampling_rate, emotion)\nspectogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#feature extraction ","metadata":{}},{"cell_type":"code","source":"def extract_mfcc(filename):\n    y, sr = librosa.load(filename, duration=3, offset=0.5)\n    mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)\n    return mfcc","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"extract_mfcc(df['speech'][0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_mfcc = df['speech'].apply(lambda x: extract_mfcc(x))\nX_mfcc","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = [x for x in X_mfcc]\nX = np.array(X)\nX.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## input split\nX = np.expand_dims(X, -1)\nX.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder()\ny = enc.fit_transform(df[['label']])\ny = y.toarray()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create the LSTM Model","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout\n\nmodel = Sequential([\n    LSTM(256, return_sequences=False, input_shape=(40,1)),\n    Dropout(0.2),\n    Dense(128, activation='relu'),\n    Dropout(0.2),\n    Dense(64, activation='relu'),\n    Dropout(0.2),\n    Dense(7, activation='softmax')\n])\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(X, y, validation_split=0.2, epochs=50, batch_size=64 , shuffle =True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Plot the results","metadata":{}},{"cell_type":"code","source":"epochs = list(range(50))\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nplt.plot(epochs, acc, label='train accuracy')\nplt.plot(epochs, val_acc, label='val accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.plot(epochs, loss, label='train loss')\nplt.plot(epochs, val_loss, label='val loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# single_file_test_keras.py\nimport numpy as np\nimport librosa\nimport tensorflow as tf\nfrom pathlib import Path\n\n# --- CONFIG (match training) ---\nMODEL_PATH = \"model.h5\"\nSAMPLE_RATE = 16000\nDURATION = 3.0            # seconds used during training\nN_MFCC = 40\nLABELS = ['neutral','happy','sad','angry']   # replace with your labels\n\n# --- helpers ---\ndef load_audio(path, sr=SAMPLE_RATE, duration=DURATION):\n    y, _ = librosa.load(path, sr=sr, mono=True, duration=duration)\n    # pad or truncate to exact duration\n    target_len = int(sr * duration)\n    if len(y) < target_len:\n        y = np.pad(y, (0, target_len - len(y)))\n    else:\n        y = y[:target_len]\n    return y\n\ndef extract_mfcc(y, sr=SAMPLE_RATE, n_mfcc=N_MFCC):\n    mf = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n    # typical shape: (n_mfcc, time_steps) -> normalize/scale if training used that\n    mf = (mf - np.mean(mf)) / (np.std(mf) + 1e-9)\n    return mf\n\n# --- load model ---\nmodel = tf.keras.models.load_model(MODEL_PATH)\n\n# --- predict single file ---\naudio_path = \"test_audio.wav\"  # change\ny = load_audio(audio_path)\nmf = extract_mfcc(y)\n# expand dims to model input shape e.g. (1, n_mfcc, time_steps, 1) or (1, time_steps, n_mfcc)\ninp = mf[np.newaxis, ..., np.newaxis]   # adapt if your model expects channels_first etc\nprobs = model.predict(inp)[0]\npred_idx = np.argmax(probs)\nprint(\"Prediction:\", LABELS[pred_idx], \"Confidence:\", float(probs[pred_idx]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# streamlit_test_app.py\nimport streamlit as st\nimport librosa, numpy as np\nimport tensorflow as tf\n\nMODEL_PATH = \"model.h5\"\nSAMPLE_RATE = 16000\nDURATION = 3.0\nN_MFCC = 40\nLABELS = ['neutral','happy','sad','angry']\n\n@st.cache_resource\ndef load_model():\n    return tf.keras.models.load_model(MODEL_PATH)\n\ndef preprocess_file(wav_bytes):\n    y, _ = librosa.load(librosa.util.example_audio_file() if wav_bytes is None else wav_bytes,\n                        sr=SAMPLE_RATE, mono=True, duration=DURATION)\n    target_len = int(SAMPLE_RATE * DURATION)\n    if len(y) < target_len:\n        y = np.pad(y, (0, target_len - len(y)))\n    else:\n        y = y[:target_len]\n    mf = librosa.feature.mfcc(y=y, sr=SAMPLE_RATE, n_mfcc=N_MFCC)\n    mf = (mf - np.mean(mf)) / (np.std(mf) + 1e-9)\n    return mf\n\nst.title(\"Speech Emotion Test\")\nuploaded = st.file_uploader(\"Upload a WAV/MP3\", type=['wav','mp3','flac'])\nmodel = load_model()\n\nif uploaded:\n    # read bytes to temp file\n    with open(\"tmp_upload.wav\",\"wb\") as f:\n        f.write(uploaded.getbuffer())\n    mf = preprocess_file(\"tmp_upload.wav\")\n    inp = mf[np.newaxis, ..., np.newaxis]\n    probs = model.predict(inp)[0]\n    pred = LABELS[np.argmax(probs)]\n    st.write(\"Prediction:\", pred)\n    st.write({LABELS[i]: float(probs[i]) for i in range(len(LABELS))})\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}